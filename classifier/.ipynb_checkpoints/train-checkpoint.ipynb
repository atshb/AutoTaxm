{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Train Classification Model for Ontology Generation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class\n",
    "- 入力データを制御するためのクラス。\n",
    "- データをvectorizerによりベクトル化\n",
    "- ベクトルと教師ラベルをtorch.tensor型に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " class OntDataset(data.Dataset):\n",
    "    def __init__(self, dataset, vectorizer, device='cpu'):\n",
    "        self.dataset = dataset\n",
    "        self.vectorizer = vectorizer\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a, b, l = self.dataset[idx]\n",
    "        a = torch.from_numpy(self.vectorizer[a]).float().to(device)\n",
    "        b = torch.from_numpy(self.vectorizer[b]).float().to(device)\n",
    "        l = torch.tensor(l).to(device)\n",
    "        return ((a, b), l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConcat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConcat, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        a, b = x\n",
    "        batch_size = a.size()[0]\n",
    "        a, b = a.reshape(batch_size, -1), b.reshape(batch_size, -1)\n",
    "        x = torch.cat([a, b], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ont(nn.Module):\n",
    "    def __init__(self, concat, x_size, h_size=300, y_size=4, drop_rate=0.5):\n",
    "        super(Ont, self).__init__()\n",
    "        self.concat = concat\n",
    "        self.l1 = nn.Linear(x_size, h_size)\n",
    "        self.l2 = nn.Linear(h_size, h_size)\n",
    "        self.l3 = nn.Linear(h_size, y_size)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.concat(x)\n",
    "        h = self.drop(F.relu(self.l1(x)))\n",
    "        h = self.drop(F.relu(self.l2(h)))\n",
    "        t = self.l3(h)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_epoch  = 50\n",
    "batch_size = 1024\n",
    "\n",
    "h_size = 512\n",
    "drop_rate = 0.2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "vectorizer = pd.read_pickle('../dataset/vectorizer_w2v.pkl')\n",
    "train_df   = pd.read_pickle('../dataset/wordnet_train.pkl')\n",
    "valid_df   = pd.read_pickle('../dataset/wordnet_valid.pkl')\n",
    "\n",
    "train_dataset = OntDataset(train_df, vectorizer, device=device)\n",
    "valid_dataset = OntDataset(valid_df, vectorizer, device=device)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Calculate input size\n",
    "max_seq_len, vec_size = vectorizer['example'].shape\n",
    "x_size = 2 * max_seq_len * vec_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ont(SimpleConcat(), x_size=x_size, h_size=h_size, drop_rate=drop_rate).to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        t = model(x)\n",
    "        loss = loss_func(t, y)\n",
    "        epoch_loss += loss.cpu().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if (i + 1) % 10 == 0: print(f'{i + 1:>4} : {loss.cpu().item():>6.3}')\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid():\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_accu = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            t = model(x)\n",
    "            _, t = torch.max(t.data, 1)\n",
    "            epoch_accu += sum(1 for t_i, y_i in zip(t, y) if t_i == y_i)\n",
    "    \n",
    "    return epoch_accu / len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 | loss : 499.772 | accu : 79.79%\n",
      "01 | loss : 448.687 | accu : 80.42%\n",
      "02 | loss : 414.161 | accu : 81.08%\n",
      "03 | loss : 387.717 | accu : 81.32%\n",
      "04 | loss : 367.570 | accu : 81.52%\n",
      "05 | loss : 350.533 | accu : 81.64%\n",
      "06 | loss : 337.531 | accu : 81.71%\n",
      "07 | loss : 326.037 | accu : 81.93%\n",
      "08 | loss : 315.160 | accu : 81.91%\n",
      "09 | loss : 305.910 | accu : 82.00%\n",
      "10 | loss : 298.562 | accu : 82.13%\n",
      "11 | loss : 291.519 | accu : 82.24%\n",
      "12 | loss : 284.738 | accu : 82.15%\n",
      "13 | loss : 278.571 | accu : 82.24%\n",
      "14 | loss : 273.338 | accu : 82.38%\n",
      "15 | loss : 268.360 | accu : 82.47%\n",
      "16 | loss : 264.086 | accu : 82.52%\n",
      "17 | loss : 259.080 | accu : 82.52%\n",
      "18 | loss : 256.658 | accu : 82.53%\n",
      "19 | loss : 251.655 | accu : 82.50%\n",
      "20 | loss : 248.480 | accu : 82.47%\n",
      "21 | loss : 244.744 | accu : 82.45%\n",
      "22 | loss : 242.098 | accu : 82.50%\n",
      "23 | loss : 239.434 | accu : 82.50%\n",
      "24 | loss : 235.964 | accu : 82.51%\n",
      "25 | loss : 233.702 | accu : 82.54%\n",
      "26 | loss : 231.351 | accu : 82.60%\n",
      "27 | loss : 229.207 | accu : 82.57%\n",
      "28 | loss : 227.569 | accu : 82.48%\n",
      "29 | loss : 224.893 | accu : 82.47%\n",
      "30 | loss : 222.708 | accu : 82.47%\n",
      "31 | loss : 220.676 | accu : 82.46%\n",
      "32 | loss : 218.774 | accu : 82.56%\n",
      "33 | loss : 217.442 | accu : 82.55%\n",
      "34 | loss : 215.482 | accu : 82.62%\n",
      "35 | loss : 213.403 | accu : 82.51%\n",
      "36 | loss : 211.889 | accu : 82.44%\n",
      "37 | loss : 210.151 | accu : 82.44%\n",
      "38 | loss : 209.724 | accu : 82.48%\n",
      "39 | loss : 206.837 | accu : 82.48%\n",
      "40 | loss : 206.140 | accu : 82.45%\n",
      "41 | loss : 205.078 | accu : 82.55%\n",
      "42 | loss : 203.167 | accu : 82.58%\n",
      "43 | loss : 201.373 | accu : 82.48%\n",
      "44 | loss : 200.390 | accu : 82.42%\n",
      "45 | loss : 199.869 | accu : 82.53%\n",
      "46 | loss : 199.016 | accu : 82.45%\n",
      "47 | loss : 197.032 | accu : 82.49%\n",
      "48 | loss : 196.007 | accu : 82.51%\n",
      "49 | loss : 195.439 | accu : 82.30%\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 50\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    loss = train()\n",
    "    accu = valid()\n",
    "    print(f'{epoch:0>2} | loss : {loss:>7.3f} | accu : {accu:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
